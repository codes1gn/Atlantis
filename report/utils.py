
import tempfile
import logging
import tvm
from e2e_perf_logger import *
from tvm.tir.tensor_intrin.cuda import get_wmma_intrin_group
from tvm import meta_schedule as ms

AUTOTVM_LOG_PATH = "autotvm.log"
# TODO(albert): resolve this hardcode, to relative path of atlantis project
BASE_PATH = "/root/AlbertResearches/researches/mortise/atlantis/records/"

TUNING_LOG = "/root/AlbertResearches/researches/mortise/atlantis/records/tune_tmp/collage/rtx3070.tuninglog"


def get_build_target(hw):
    HOST = tvm.target.Target("llvm")
    CUDA = tvm.target.Target("cuda", HOST)
    if hw == "cpu":
        return str(HOST)
    elif hw == "gpu":
        return str(CUDA)
    else:
        raise ValueError("invalid target" + hw)


def get_autotvm_log_path(hw_name):
    return BASE_PATH + hw_name + "-" + AUTOTVM_LOG_PATH


def log_e2e_perf(args, method, mean_perf, std_perf, is_perf_logging):
    if is_perf_logging:
        E2EPerfLogger().log_perf(args.hw, args.batch_size,
                                 args.network, method, mean_perf, std_perf)
        if args.tuning == "true":
            DPTuningTimeLogger().log_perf(args.hw, args.network, method, args.tune_time, 0)


def log_afusion_perf(args, method, mean_perf, compile_time, is_perf_logging):
    if is_perf_logging:
        AfusionPerfLogger().log_perf(args.func, args.hw, args.batch_size,
                                     args.network, method, mean_perf, compile_time)
        if args.tuning == "true":
            AfusionPerfLogger().log_perf(args.hw, args.network, method, mean_perf, compile_time)


def multi_level_tiling_tensor_core(
    *,
    read_reuse_scope="shared",
    write_reuse_scope="shared",
    in_dtype="float16",
    out_dtype="float32",
    trans_b=False,
    use_software_pipeline=False,
) -> ms.schedule_rule.ScheduleRule:
    assert read_reuse_scope in ["shared", "shared.dyn"]
    assert write_reuse_scope in ["shared", "shared.dyn", "global"]
    if not isinstance(in_dtype, list):
        in_dtype = [in_dtype]
    if not isinstance(out_dtype, list):
        out_dtype = [out_dtype]
    if not isinstance(trans_b, list):
        trans_b = [trans_b]
    return ms.schedule_rule.MultiLevelTilingTensorCore(
        intrin_groups=[
            get_wmma_intrin_group(
                read_reuse_scope, write_reuse_scope, _in_dtype, _out_dtype, _trans_b
            )
            for _in_dtype in in_dtype
            for _out_dtype in out_dtype
            for _trans_b in trans_b
        ],
        structure="SSSRRSRS",
        tile_binds=["blockIdx.y", "blockIdx.x", "threadIdx.y"],
        max_innermost_factor=4,  # 64 // tensor intrin size
        vector_load_lens=[1, 2, 3, 4, 8, 16],
        reuse_read=ms.schedule_rule.ReuseType(
            req="must",
            levels=[4],
            scope=read_reuse_scope,
        ),
        reuse_write=ms.schedule_rule.ReuseType(
            req="must" if write_reuse_scope.startswith("shared") else "no",
            levels=[2],
            scope=write_reuse_scope,
        ),
        use_software_pipeline=use_software_pipeline,
    )


# collage
logging.basicConfig(level=logging.INFO)
PROFILE = True
HOST = "llvm"
TVM_MAX_DEPTH = 8
BYOC_MAX_DEPTH = 8
MEASURE_NUMBER = tvm.relay.collage.MEASURE_NUMBER
MEASURE_REPEAT = tvm.relay.collage.MEASURE_REPEAT
WARMUP_MIN_REPEAT_MS = tvm.relay.collage.WARMUP_MIN_REPEAT_MS

runner_template = f"""
import tvm
import tvm.runtime.vm
import numpy as np
import logging

logging.basicConfig(level=logging.INFO)

MEASURE_NUMBER = {MEASURE_NUMBER}
MEASURE_REPEAT = {MEASURE_REPEAT}
WARMUP_MIN_REPEAT_MS = {WARMUP_MIN_REPEAT_MS}

def arg_for(shape, dtype, device):
    return tvm.nd.array(
        np.random.rand(*shape).astype(dtype), device=device)

def vm_estimate_seconds(device, vm, args):
    vm.benchmark(device, repeat=1, number=1, min_repeat_ms=WARMUP_MIN_REPEAT_MS, **args)
    return vm.benchmark(device, repeat=MEASURE_REPEAT, number=MEASURE_NUMBER, min_repeat_ms=0,
                        **args)


def run(label, name, device, lib_path, code_path, input_shapes, input_dtypes):
    logging.info(f"Loading compiled code for {{name}} generated by {{label}} from {{lib_path}} and {{code_path}}...")
    loaded_lib = tvm.runtime.load_module(lib_path)
    loaded_code = bytearray(open(code_path, "rb").read())
    loaded_exe = tvm.runtime.vm.Executable.load_exec(loaded_code, loaded_lib)
    vm = tvm.runtime.vm.VirtualMachine(loaded_exe, device)
    args = {{
        input_name: arg_for(input_shapes[input_name], input_dtypes[input_name], device)
        for input_name in input_shapes.keys()
    }}
    logging.info(f"Benchmarking for {{name}} generated by {{label}}...")
    profile = vm_estimate_seconds(device, vm, args)
    logging.info(f"Benchmarked for {{name}} generated by {{label}}: {{profile}}")
    logging.info(f"RESULT: {{label}} | {{name}} | {{profile.median * 1e3}}ms")

if __name__ == "__main__":
"""


def compile_and_benchmark(label, model, targets, dev, tmp_dir):
    """Compile model for target and run it with profiling."""
    logging.info(f"Compiling {model['name']} using {label} with {targets}...")
    exe = tvm.relay.vm.compile(
        model["mod"], target=targets, params=model["params"])
    lib_path = os.path.join(tmp_dir, "lib.so")
    code_path = os.path.join(tmp_dir, "code.ro")
    code, lib = exe.save()
    logging.info(f"Saving VM code to {code_path}...")
    with open(code_path, "wb") as fo:
        fo.write(code)
    logging.info(f"Exporting library to {lib_path}...")
    lib.export_library(lib_path, workspace_dir=tmp_dir, cc="nvcc")
    runner = f"{runner_template}    run('{label}', '{model['name']}', tvm.device({dev.device_type}), '{lib_path}', '{code_path}', {model['input_shapes']}, {model['input_dtypes']})\n"
    runner_path = os.path.join(tmp_dir, "runner.py")
    logging.info(f"Saving runner to {runner_path}...")
    with open(runner_path, "w") as fo:
        fo.write(runner)

    logging.info(f"Invoking runner...")
    if PROFILE:
        profile_path = os.path.join(tmp_dir, "profile.txt")
        os.system(f"nsys nvprof -o {profile_path} python3 {runner_path}")
    else:
        os.system(f"python3 {runner_path}")


def optional_tuning_records(log_filename):
    """Returns existing tuning records, if any."""
    if log_filename == "" or not os.path.exists(log_filename):
        return tvm.autotvm.task.FallbackContext()
    else:
        return tvm.autotvm.task.ApplyHistoryBest(log_filename)


def autotvm_tune_module(mod, target, log_filename):
    if log_filename == "":
        logging.info("Not tuning with autotvm since disabled")
        return
    # Extract and tune any TVM kernels. BYOC partitions will have no tasks extracted.
    logging.info("Extracting tasks from overall module")
    tasks = extract_autotvm_tasks(mod, target)
    logging.info(f"Auto-tuning {len(tasks)} tasks from overall module")
    tune_autotvm_tasks(tasks, log_filename)


def collage(net, name, target, dtype):
    import logging
    import tempfile
    logging.basicConfig(level=logging.INFO)
    """Run the Collage partitioner for a set of CUDA-related targets and profile the result"""
    logging.info(f"collage | {name}")
    # logging.info("-------------- BEGIN ORIGINAL --------------")
    # logging.info(model["mod"])
    # logging.info("-------------- END ORIGINAL ----------------")
    autotvm_tune_module(model["mod"], CUDA, TUNING_LOG)
    with optional_tuning_records(TUNING_LOG):
        targets = []
        targets.append(target)
        use_fp16 = dtype == "float16"
        targets.append(
            tvm.target.Target(
                f"tensorrt -use_implicit_batch=False -use_fp16={use_fp16}", HOST)
        )
        tmp_dir = tempfile.mkdtemp()
        targets.append(tvm.target.Target(f"cutlass -tmp_dir={tmp_dir}", HOST))
        targets.append(tvm.target.Target("cublas", HOST))
        targets.append(tvm.target.Target("cudnn", HOST))
        config = {
            "relay.collage.tvm_max_depth": TVM_MAX_DEPTH,
            "relay.collage.byoc_max_depth": BYOC_MAX_DEPTH,
            "relay.collage.byoc_fusion_style": [
                "cutlass.NoFusion",
                "cublas.NoFusion",
                "cudnn.NoFusion",
                "tensorrt.TVMFusion",
            ],
        }
        logging.info(f"Using PassContext(config={config}")
        ctxt = tvm.transform.PassContext(config=config)
        config = tvm.target.make_compilation_config(ctxt, targets)
        with ctxt:
            mod = net
            mod = tvm.relay.transform.CapturePostDfsIndexInSpans()(mod)
            logging.info("-------------- BEGIN INDEXED --------------")
            logging.info(mod)
            logging.info("-------------- END INDEXED ----------------")
            mod = tvm.relay.transform.CollagePartition(config)(mod)
            # partitioned_model = net.copy()
            partitioned_model["mod"] = mod
            partitioned_model["name"] = name
            logging.info("-------------- BEGIN PARTITIONED --------------")
            logging.info(partitioned_model["mod"])
            logging.info("-------------- END PARTITIONED ----------------")
            dev = tvm.device(target, 0)
            compile_and_benchmark(
                "collage", partitioned_model, targets, dev, tmp_dir)
