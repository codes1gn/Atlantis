{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import fx\n",
    "from torch.nn import Module\n",
    "import time\n",
    "import tvm\n",
    "from tvm import relax\n",
    "import tvm.testing\n",
    "from tvm.script import ir as I\n",
    "from tvm.script import relax as R\n",
    "from tvm.script import tir as T\n",
    "from tvm import relay\n",
    "from tvm.relay.testing import *\n",
    "from tvm.relay.testing import layers\n",
    "from tvm.relay.testing.resnet import *\n",
    "from tvm.contrib import graph_executor\n",
    "from tvm import relax\n",
    "from tvm.relax import testing\n",
    "from tvm.relax.testing import relay_translator, nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "units = [48//6] * 3\n",
    "filter_list = [16, 16, 32, 64]\n",
    "num_stages=3\n",
    "bottle_neck = False\n",
    "shape =( 1,3,224,224)\n",
    "layout=\"NCHW\"\n",
    "dtype=\"float32\"\n",
    "kernel_layout = \"OIHW\" if layout == \"NCHW\" else \"HWIO\"\n",
    "bn_axis = layout.index(\"C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get3D():\n",
    "    body = relay.var(\"data\", shape=(1,1,3,224,224), dtype=dtype)\n",
    "    for i in range(25):\n",
    "        body = layers.conv3d(\n",
    "                data=body,\n",
    "                channels=filter_list[0],\n",
    "                kernel_size=(3, 7, 7),\n",
    "                strides=(1, 2, 2),\n",
    "                padding=(1, 3, 3),\n",
    "                name=\"conv0\",\n",
    "                data_layout=\"NCDHW\",\n",
    "                kernel_layout=\"OIDHW\",\n",
    "            )\n",
    "        body = layers.batch_norm_infer(data=body, epsilon=2e-5, name=\"bn0\")\n",
    "        body = relay.nn.relu(data=body)\n",
    "    f = relay.Function(relay.analysis.free_vars(body), body)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCBR():\n",
    "    name = \"a1\"\n",
    "    data = relay.var(\"data\", shape=(1,3,224,224), dtype=dtype)\n",
    "    body = layers.conv2d(\n",
    "            data=data,\n",
    "            channels=filter_list[0],\n",
    "            kernel_size=(7, 7),\n",
    "            strides=(2, 2),\n",
    "            padding=(3, 3),\n",
    "            name=\"conv0\",\n",
    "            data_layout=layout,\n",
    "            kernel_layout=kernel_layout,\n",
    "        )\n",
    "    body = layers.batch_norm_infer(data=body, epsilon=2e-5, axis=bn_axis, name=\"bn0\")\n",
    "    body = relay.nn.relu(data=body)\n",
    "    body = relay.nn.max_pool2d(\n",
    "                data=body, pool_size=(3, 3), strides=(2, 2), padding=(1, 1), layout=layout\n",
    "            )\n",
    "    for i in range(100):\n",
    "        body = residual_unit(\n",
    "                    body,\n",
    "                    filter_list[0],\n",
    "                    (1, 1),\n",
    "                    True,\n",
    "                    name=f\"stage{i + 1}_unit{i + 2}\",\n",
    "                    bottle_neck=bottle_neck,\n",
    "                    data_layout=layout,\n",
    "                    kernel_layout=kernel_layout,\n",
    "                )\n",
    "\n",
    "    f = relay.Function(relay.analysis.free_vars(body), body)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCAR():\n",
    "    name = \"a2\"\n",
    "    body = relay.var(\"data\", shape=(1,3,224,224), dtype=dtype)\n",
    "   \n",
    "    # body = layers.batch_norm_infer(data=body, epsilon=2e-5, axis=bn_axis, name=\"bn0\")\n",
    "    body = relay.nn.relu(data=body)\n",
    "    # body = relay.nn.max_pool2d(\n",
    "    #             data=body, pool_size=(3, 3), strides=(2, 2), padding=(1, 1), layout=layout\n",
    "    #         )\n",
    "    for i in range(50):\n",
    "        body = layers.conv2d(\n",
    "            data=body,\n",
    "            channels=filter_list[0],\n",
    "            kernel_size=(7, 7),\n",
    "            strides=(2, 2),\n",
    "            padding=(3, 3),\n",
    "            name=\"conv0\",\n",
    "            data_layout=layout,\n",
    "            kernel_layout=kernel_layout,\n",
    "        )\n",
    "        body = relay.nn.bias_add(body,relay.var(\"conv1_bias\"))\n",
    "        body = relay.nn.relu(body)\n",
    "\n",
    "    f = relay.Function(relay.analysis.free_vars(body), body)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCR():\n",
    "    name = \"a2\"\n",
    "    body = relay.var(\"data\", shape=(1,3,224,224), dtype=dtype)\n",
    "   \n",
    "    # body = layers.batch_norm_infer(data=body, epsilon=2e-5, axis=bn_axis, name=\"bn0\")\n",
    "    body = relay.nn.relu(data=body)\n",
    "    # body = relay.nn.max_pool2d(\n",
    "    #             data=body, pool_size=(3, 3), strides=(2, 2), padding=(1, 1), layout=layout\n",
    "    #         )\n",
    "    for i in range(50):\n",
    "        body = layers.conv2d(\n",
    "            data=body,\n",
    "            channels=filter_list[0],\n",
    "            kernel_size=(7, 7),\n",
    "            strides=(2, 2),\n",
    "            padding=(3, 3),\n",
    "            name=\"conv0\",\n",
    "            data_layout=layout,\n",
    "            kernel_layout=kernel_layout,\n",
    "        )\n",
    "        body = relay.nn.relu(body)\n",
    "\n",
    "    f = relay.Function(relay.analysis.free_vars(body), body)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDAB():\n",
    "    data = relay.var(\"data\", relay.TensorType((1,  224), dtype))\n",
    "   \n",
    "    body = layers.dense_add_bias(data=data, units=4096, name=\"fc6\")\n",
    "    for i in range(25):\n",
    "        body = layers.dense_add_bias(body, units=4096, name=\"fc6\")\n",
    "\n",
    "    f = relay.Function(relay.analysis.free_vars(body), body)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDABR():\n",
    "    data = relay.var(\"data\", relay.TensorType((1,  224), dtype))\n",
    "   \n",
    "    body = layers.dense_add_bias(data=data, units=4096, name=\"fc6\")\n",
    "    for i in range(50):\n",
    "        body = layers.dense_add_bias(body, units=4096, name=\"fc6\")\n",
    "        body = relay.nn.relu(body)\n",
    "\n",
    "    f = relay.Function(relay.analysis.free_vars(body), body)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = get3D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (1,3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "501.6899684"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with tvm.transform.PassContext(opt_level=0):\n",
    "#     lib = relay.build(mod, \"llvm\")\n",
    "\n",
    "\n",
    "\n",
    "# dev = tvm.cpu(0)\n",
    "# dtype = \"float32\"\n",
    "# m = graph_executor.GraphModule(lib[\"default\"](dev))\n",
    "# m.set_input(\"data\", tvm.nd.array(np.random.randn(1,112)))\n",
    "# m.module.time_evaluator(\"run\", tvm.cpu())().mean * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tvm.transform.PassContext(opt_level=0):\n",
    "    lib = relay.build(mod, \"cuda\")\n",
    "\n",
    "dev = tvm.cuda(0)\n",
    "dtype = \"float32\"\n",
    "m = graph_executor.GraphModule(lib[\"default\"](dev))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.64254759999999"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.set_input(\"data\", tvm.nd.array(np.random.randn(1,1, 3,224,112)))\n",
    "m.module.time_evaluator(\"run\", tvm.cuda())().mean*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.81684699999998"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.module.time_evaluator(\"run\", tvm.cpu())().mean*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "relax_mod = relay_translator.from_relay(mod, \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2055454"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tvm.target.Target(\"cuda\"):\n",
    "    relax_mod = relax.transform.LegalizeOps()(relax_mod)\n",
    "    relax_mod = tvm.tir.transform.DefaultGPUSchedule()(relax_mod)\n",
    "        # seq = tvm.transform.Sequential(\n",
    "        #     [relax.transform.LegalizeOps(), tvm.tir.transform.DefaultGPUSchedule()]\n",
    "        # )\n",
    "        # relax_mod = seq(relax_mod)\n",
    "\n",
    "ex = relax.build(relax_mod, \"cuda\")\n",
    "vm = relax.VirtualMachine(ex, tvm.cuda())\n",
    "\n",
    "data = tvm.nd.array(np.random.randn(1,1,3,224,224).astype(np.float32))\n",
    "params = nn.init_params(relax_mod)\n",
    "\n",
    "vm.save_function(\"main\",\"base_func\", data, *params)\n",
    "vm.time_evaluator(\"base_func\", tvm.cuda())().mean * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1105678"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vm.time_evaluator(\"base_func\", tvm.cpu())().mean * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "  # relay_mod = relay.transform.SimplifyInference()(mod)\n",
    "  # relay_mod = relay.transform.FoldConstant()(mod)\n",
    "  # relay_mod = relay.transform.FoldScaleAxis()(relay_mod)\n",
    "  # relay_mod = relay.transform.CanonicalizeOps()(relay_mod)\n",
    "  # relay_mod = relay.transform.AlterOpLayout()(relay_mod)\n",
    "  # relay_mod = relay.transform.FoldConstant()(relay_mod)\n",
    "  \n",
    "  relax_mod = relay_translator.from_relay(mod, \"cuda\")\n",
    "#   relax_mod = relax.transform.AnnotateTIROpPattern()(relax_mod)\n",
    "  relax_mod = relax.transform.FuseOps()(relax_mod)\n",
    "#   relax_mod = relax.transform.FuseTIR()(relax_mod)\n",
    "#   relax_mod = relax.transform.DecomposeOpsForInference()(relax_mod)\n",
    "#   relax_mod = relax.transform.LegalizeOps()(relax_mod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm import dlight as dl\n",
    "from tvm.dlight.benchmark import (\n",
    "    benchmark,\n",
    "    benchmark_prim_func,\n",
    "    benchmark_relax_func,\n",
    "    extract_prim_func,\n",
    "    extract_from_relax,\n",
    "    extract_func_info_from_prim_func,\n",
    ")\n",
    "\n",
    "# with tvm.target.Target(\"cuda\"):\n",
    "#     benchmark_relax_func(relax_mod, \"main\")\n",
    "\n",
    "with tvm.target.Target(\"cuda\"):\n",
    "    d_cuda_mod = dl.ApplyDefaultSchedule(\n",
    "        # dl.gpu.Matmul(),\n",
    "        # dl.gpu.Transpose(),\n",
    "        dl.gpu.Reduction(),\n",
    "        # dl.gpu.Transpose(),\n",
    "        dl.gpu.DecodeGEMV(),\n",
    "        # dl.gpu.Matmul(),\n",
    "        # dl.gpu.Fallback(),\n",
    "    )(relax_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TVMError",
     "evalue": "Traceback (most recent call last):\n  5: 0x000055e1dfa4e250\n  4: operator()\n        at /root/wang/tvm/src/driver/driver_api.cc:514\n  3: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)\n        at /root/wang/tvm/src/driver/driver_api.cc:475\n  2: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)\n        at /root/wang/tvm/src/driver/driver_api.cc:419\n  1: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)\n        at /root/wang/tvm/src/driver/driver_api.cc:290\n  0: operator()\n        at /root/wang/tvm/src/tir/analysis/verify_memory.cc:205\n  Did you forget to bind?\n    Variable `T_relu` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n    Variable `bn0_beta` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n    Variable `lv2` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n    Variable `bn0_moving_mean` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n    Variable `lv6` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n    Variable `data` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n  File \"/root/wang/tvm/src/tir/analysis/verify_memory.cc\", line 205\nRuntimeError: Memory verification failed with the following errors:\n# from tvm.script import tir as T\n\n@T.prim_func\ndef fused_conv3d_negative_multiply_add1_expand_dims_expand_dims2_add2_relu(data: T.Buffer((T.int64(1), T.int64(1), T.int64(3), T.int64(224), T.int64(224)), \"float32\"), lv6: T.Buffer((T.int64(16), T.int64(1), T.int64(3), T.int64(7), T.int64(7)), \"float32\"), bn0_moving_mean: T.Buffer((T.int64(16),), \"float32\"), lv2: T.Buffer((T.int64(16),), \"float32\"), bn0_beta: T.Buffer((T.int64(16),), \"float32\"), var_T_relu_intermediate: T.Buffer((T.int64(1), T.int64(16), T.int64(3), T.int64(112), T.int64(112)), \"float32\")):\n    T.func_attr({\"global_symbol\": \"fused_conv3d_negative_multiply_add1_expand_dims_expand_dims2_add2_relu\", \"target\": T.target({\"arch\": \"sm_86\", \"host\": {\"keys\": [\"cpu\"], \"kind\": \"llvm\", \"tag\": \"\"}, \"keys\": [\"cuda\", \"gpu\"], \"kind\": \"cuda\", \"max_num_threads\": 1024, \"tag\": \"\", \"thread_warp_size\": 32}), \"tir.noalias\": T.bool(True)})\n    pad_temp = T.allocate([264500], \"float32\", \"global\")\n    conv3d_ncdhw = T.allocate([602112], \"float32\", \"global\")\n    T_negative = T.allocate([16], \"float32\", \"global\")\n    pad_temp_1 = T.Buffer((T.int64(264500),), data=pad_temp)\n    for i2, i3, i4 in T.grid(5, 230, 230):\n        data_1 = T.Buffer((T.int64(150528),), data=data.data)\n        pad_temp_1[i2 * 52900 + i3 * 230 + i4] = T.if_then_else(1 <= i2 and i2 < 4 and 3 <= i3 and i3 < 227 and 3 <= i4 and i4 < 227, data_1[i2 * 50176 + i3 * 224 + i4 - 50851], T.float32(0))\n    conv3d_ncdhw_1 = T.Buffer((T.int64(602112),), data=conv3d_ncdhw)\n    for ff, yy, xx, zz, ry, rx, rz in T.grid(16, 3, 112, 112, 3, 7, 7):\n        cse_var_1: T.int32 = ff * 37632 + yy * 12544 + xx * 112 + zz\n        if ry == 0 and rx == 0 and rz == 0:\n            conv3d_ncdhw_1[cse_var_1] = T.float32(0)\n        lv6_1 = T.Buffer((T.int64(2352),), data=lv6.data)\n        conv3d_ncdhw_1[cse_var_1] = conv3d_ncdhw_1[cse_var_1] + pad_temp_1[yy * 52900 + ry * 52900 + xx * 460 + rx * 230 + zz * 2 + rz] * lv6_1[ff * 147 + ry * 49 + rx * 7 + rz]\n    T_negative_1 = T.Buffer((T.int64(16),), data=T_negative)\n    for ax0 in range(16):\n        bn0_moving_mean_1 = T.Buffer((T.int64(16),), data=bn0_moving_mean.data)\n        T_negative_1[ax0] = T.float32(0) - bn0_moving_mean_1[ax0]\n    T_negative_2 = T.Buffer((T.int64(16),), data=T_negative)\n    for ax0 in range(16):\n        lv2_1 = T.Buffer((T.int64(16),), data=lv2.data)\n        T_negative_2[ax0] = T_negative_1[ax0] * lv2_1[ax0]\n    for ax0 in range(16):\n        T_negative_3 = T.Buffer((T.int64(16),), data=T_negative)\n        bn0_beta_1 = T.Buffer((T.int64(16),), data=bn0_beta.data)\n        T_negative_3[ax0] = T_negative_2[ax0] + bn0_beta_1[ax0]\n    conv3d_ncdhw_2 = T.Buffer((T.int64(602112),), data=conv3d_ncdhw)\n    for ax1, ax2, ax3, ax4 in T.grid(16, 3, 112, 112):\n        cse_var_2: T.int32 = ax1 * 37632 + ax2 * 12544 + ax3 * 112 + ax4\n        T_negative_3 = T.Buffer((T.int64(16),), data=T_negative)\n        conv3d_ncdhw_2[cse_var_2] = conv3d_ncdhw_1[cse_var_2] + T_negative_3[ax1]\n    for ax1, ax2, ax3, ax4 in T.grid(16, 3, 112, 112):\n        cse_var_3: T.int32 = ax1 * 37632 + ax2 * 12544 + ax3 * 112 + ax4\n        var_T_relu_intermediate_1 = T.Buffer((T.int64(602112),), data=var_T_relu_intermediate.data)\n        var_T_relu_intermediate_1[cse_var_3] = T.max(conv3d_ncdhw_2[cse_var_3], T.float32(0))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTVMError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ex \u001b[39m=\u001b[39m relax\u001b[39m.\u001b[39;49mbuild(d_cuda_mod, \u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m vm \u001b[39m=\u001b[39m relax\u001b[39m.\u001b[39mVirtualMachine(ex, tvm\u001b[39m.\u001b[39mcuda())\n\u001b[1;32m      4\u001b[0m shape \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m224\u001b[39m,\u001b[39m224\u001b[39m)\n",
      "File \u001b[0;32m~/wang/tvm/python/tvm/relax/vm_build.py:338\u001b[0m, in \u001b[0;36mbuild\u001b[0;34m(mod, target, params, exec_mode, system_lib)\u001b[0m\n\u001b[1;32m    336\u001b[0m leftover_mod \u001b[39m=\u001b[39m _vmcodegen(builder, new_mod, exec_mode\u001b[39m=\u001b[39mexec_mode)\n\u001b[1;32m    337\u001b[0m tir_mod \u001b[39m=\u001b[39m _filter_tir(leftover_mod)\n\u001b[0;32m--> 338\u001b[0m \u001b[39mreturn\u001b[39;00m _vmlink(builder, target, tir_mod, ext_libs, params, system_lib\u001b[39m=\u001b[39;49msystem_lib)\n",
      "File \u001b[0;32m~/wang/tvm/python/tvm/relax/vm_build.py:242\u001b[0m, in \u001b[0;36m_vmlink\u001b[0;34m(builder, target, tir_mod, ext_libs, params, system_lib)\u001b[0m\n\u001b[1;32m    240\u001b[0m lib \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[39mif\u001b[39;00m tir_mod \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 242\u001b[0m     lib \u001b[39m=\u001b[39m tvm\u001b[39m.\u001b[39;49mbuild(\n\u001b[1;32m    243\u001b[0m         tir_mod, target\u001b[39m=\u001b[39;49mtarget, runtime\u001b[39m=\u001b[39;49m_autodetect_system_lib_req(target, system_lib)\n\u001b[1;32m    244\u001b[0m     )\n\u001b[1;32m    245\u001b[0m \u001b[39mreturn\u001b[39;00m Executable(_ffi_api\u001b[39m.\u001b[39mVMLink(builder, target, lib, ext_libs, params))\n",
      "File \u001b[0;32m~/wang/tvm/python/tvm/driver/build_module.py:281\u001b[0m, in \u001b[0;36mbuild\u001b[0;34m(inputs, args, target, target_host, runtime, name, binds)\u001b[0m\n\u001b[1;32m    277\u001b[0m     target_host \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mllvm\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m tvm\u001b[39m.\u001b[39mruntime\u001b[39m.\u001b[39menabled(\u001b[39m\"\u001b[39m\u001b[39mllvm\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mstackvm\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m annotated_mods, target_host \u001b[39m=\u001b[39m Target\u001b[39m.\u001b[39mcanon_target_map_and_host(annotated_mods, target_host)\n\u001b[0;32m--> 281\u001b[0m rt_mod_host \u001b[39m=\u001b[39m _driver_ffi\u001b[39m.\u001b[39;49mtir_to_runtime(annotated_mods, target_host)\n\u001b[1;32m    283\u001b[0m annotated_mods, target_host \u001b[39m=\u001b[39m Target\u001b[39m.\u001b[39mcanon_target_map_and_host(annotated_mods, target_host)\n\u001b[1;32m    285\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(target_host, Target):\n",
      "File \u001b[0;32m~/wang/tvm/python/tvm/_ffi/_ctypes/packed_func.py:238\u001b[0m, in \u001b[0;36mPackedFuncBase.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    226\u001b[0m ret_tcode \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mc_int()\n\u001b[1;32m    227\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    228\u001b[0m     _LIB\u001b[39m.\u001b[39mTVMFuncCall(\n\u001b[1;32m    229\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    237\u001b[0m ):\n\u001b[0;32m--> 238\u001b[0m     \u001b[39mraise\u001b[39;00m get_last_ffi_error()\n\u001b[1;32m    239\u001b[0m _ \u001b[39m=\u001b[39m temp_args\n\u001b[1;32m    240\u001b[0m _ \u001b[39m=\u001b[39m args\n",
      "\u001b[0;31mTVMError\u001b[0m: Traceback (most recent call last):\n  5: 0x000055e1dfa4e250\n  4: operator()\n        at /root/wang/tvm/src/driver/driver_api.cc:514\n  3: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)\n        at /root/wang/tvm/src/driver/driver_api.cc:475\n  2: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)\n        at /root/wang/tvm/src/driver/driver_api.cc:419\n  1: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)\n        at /root/wang/tvm/src/driver/driver_api.cc:290\n  0: operator()\n        at /root/wang/tvm/src/tir/analysis/verify_memory.cc:205\n  Did you forget to bind?\n    Variable `T_relu` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n    Variable `bn0_beta` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n    Variable `lv2` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n    Variable `bn0_moving_mean` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n    Variable `lv6` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n    Variable `data` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n  File \"/root/wang/tvm/src/tir/analysis/verify_memory.cc\", line 205\nRuntimeError: Memory verification failed with the following errors:\n# from tvm.script import tir as T\n\n@T.prim_func\ndef fused_conv3d_negative_multiply_add1_expand_dims_expand_dims2_add2_relu(data: T.Buffer((T.int64(1), T.int64(1), T.int64(3), T.int64(224), T.int64(224)), \"float32\"), lv6: T.Buffer((T.int64(16), T.int64(1), T.int64(3), T.int64(7), T.int64(7)), \"float32\"), bn0_moving_mean: T.Buffer((T.int64(16),), \"float32\"), lv2: T.Buffer((T.int64(16),), \"float32\"), bn0_beta: T.Buffer((T.int64(16),), \"float32\"), var_T_relu_intermediate: T.Buffer((T.int64(1), T.int64(16), T.int64(3), T.int64(112), T.int64(112)), \"float32\")):\n    T.func_attr({\"global_symbol\": \"fused_conv3d_negative_multiply_add1_expand_dims_expand_dims2_add2_relu\", \"target\": T.target({\"arch\": \"sm_86\", \"host\": {\"keys\": [\"cpu\"], \"kind\": \"llvm\", \"tag\": \"\"}, \"keys\": [\"cuda\", \"gpu\"], \"kind\": \"cuda\", \"max_num_threads\": 1024, \"tag\": \"\", \"thread_warp_size\": 32}), \"tir.noalias\": T.bool(True)})\n    pad_temp = T.allocate([264500], \"float32\", \"global\")\n    conv3d_ncdhw = T.allocate([602112], \"float32\", \"global\")\n    T_negative = T.allocate([16], \"float32\", \"global\")\n    pad_temp_1 = T.Buffer((T.int64(264500),), data=pad_temp)\n    for i2, i3, i4 in T.grid(5, 230, 230):\n        data_1 = T.Buffer((T.int64(150528),), data=data.data)\n        pad_temp_1[i2 * 52900 + i3 * 230 + i4] = T.if_then_else(1 <= i2 and i2 < 4 and 3 <= i3 and i3 < 227 and 3 <= i4 and i4 < 227, data_1[i2 * 50176 + i3 * 224 + i4 - 50851], T.float32(0))\n    conv3d_ncdhw_1 = T.Buffer((T.int64(602112),), data=conv3d_ncdhw)\n    for ff, yy, xx, zz, ry, rx, rz in T.grid(16, 3, 112, 112, 3, 7, 7):\n        cse_var_1: T.int32 = ff * 37632 + yy * 12544 + xx * 112 + zz\n        if ry == 0 and rx == 0 and rz == 0:\n            conv3d_ncdhw_1[cse_var_1] = T.float32(0)\n        lv6_1 = T.Buffer((T.int64(2352),), data=lv6.data)\n        conv3d_ncdhw_1[cse_var_1] = conv3d_ncdhw_1[cse_var_1] + pad_temp_1[yy * 52900 + ry * 52900 + xx * 460 + rx * 230 + zz * 2 + rz] * lv6_1[ff * 147 + ry * 49 + rx * 7 + rz]\n    T_negative_1 = T.Buffer((T.int64(16),), data=T_negative)\n    for ax0 in range(16):\n        bn0_moving_mean_1 = T.Buffer((T.int64(16),), data=bn0_moving_mean.data)\n        T_negative_1[ax0] = T.float32(0) - bn0_moving_mean_1[ax0]\n    T_negative_2 = T.Buffer((T.int64(16),), data=T_negative)\n    for ax0 in range(16):\n        lv2_1 = T.Buffer((T.int64(16),), data=lv2.data)\n        T_negative_2[ax0] = T_negative_1[ax0] * lv2_1[ax0]\n    for ax0 in range(16):\n        T_negative_3 = T.Buffer((T.int64(16),), data=T_negative)\n        bn0_beta_1 = T.Buffer((T.int64(16),), data=bn0_beta.data)\n        T_negative_3[ax0] = T_negative_2[ax0] + bn0_beta_1[ax0]\n    conv3d_ncdhw_2 = T.Buffer((T.int64(602112),), data=conv3d_ncdhw)\n    for ax1, ax2, ax3, ax4 in T.grid(16, 3, 112, 112):\n        cse_var_2: T.int32 = ax1 * 37632 + ax2 * 12544 + ax3 * 112 + ax4\n        T_negative_3 = T.Buffer((T.int64(16),), data=T_negative)\n        conv3d_ncdhw_2[cse_var_2] = conv3d_ncdhw_1[cse_var_2] + T_negative_3[ax1]\n    for ax1, ax2, ax3, ax4 in T.grid(16, 3, 112, 112):\n        cse_var_3: T.int32 = ax1 * 37632 + ax2 * 12544 + ax3 * 112 + ax4\n        var_T_relu_intermediate_1 = T.Buffer((T.int64(602112),), data=var_T_relu_intermediate.data)\n        var_T_relu_intermediate_1[cse_var_3] = T.max(conv3d_ncdhw_2[cse_var_3], T.float32(0))"
     ]
    }
   ],
   "source": [
    "ex = relax.build(d_cuda_mod, \"cuda\")\n",
    "vm = relax.VirtualMachine(ex, tvm.cuda())\n",
    "\n",
    "shape = (1, 1,3,224,224)\n",
    "data = tvm.nd.array(np.random.rand(*shape).astype(np.float32))\n",
    "params = nn.init_params(d_cuda_mod)\n",
    "\n",
    "vm.save_function(\"main\",\"base_func\", data, *params)\n",
    "vm.time_evaluator(\"base_func\", tvm.cuda())().mean * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.4727045"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vm.time_evaluator(\"base_func\", tvm.cpu())().mean * 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
